# 20-04-15

# 一、WordCount错误原因及解决方案

## 1. 错误总结

![Snipaste_2020-04-15_08-53-40](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/Snipaste_2020-04-15_08-53-40.png)



## 2. 类型匹配错误

1.类型写错了

2.如果保留父类方法,则进入调用,则直接往里面写数据了

在map万法中避留了super.map（ key,value, context）

![image-20200415084924435](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415084924435.png)

父类的map方法直接往里面写数据了

![image-20200415084845566](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415084845566.png)



## 3. 环境问题

安装c+组件， hadoop.dll放到c:/ windows/ system32等



# 二、WordCount集群测试

## 1. 本地改集群(Linux)

集群运行,更改为HDFS的路径

![image-20200415091711707](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415091711707.png)

打包

![image-20200415091834203](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415091834203.png)

启动集群

![image-20200415092030801](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092030801.png)

![image-20200415092103473](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092103473.png)

上传Maven生成的jar包wc.jar

上传源文件

![image-20200415092432815](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092432815.png)

查看结果

![image-20200415092454140](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092454140.png)



## 2. 本地改集群(Win)

### 2.1 添加conf

在获取conf之前添加具体的集群设置\

![image-20200415092904122](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092904122.png)

### 2.2 添加jar包

可同上,上传到Linux去运行

也使用maven-package打成一个jar包,win的jar可以直接提交到Yarn上去执行

![image-20200415093016770](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093016770.png)

![image-20200415093032784](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093032784.png)

### 2.3 修改配置

(run/debug configuration)

![image-20200415093309248](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093309248.png)

![image-20200415093123065](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093123065.png)

vm options:谁来操作这个集群,后面传入参数为

### 2.4 输出结果

![image-20200415093343889](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093343889.png)

## ***多敲几遍*** ***多敲几遍*** ***多敲几遍***!!!



# 二、序列化

## 1. 基础概念

- 序列化是什么

序列化:对象->字节,把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 

反序列化:字节->对象,将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。

- 为什么要序列化

 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。

- 为什么不用Java的序列化

减少了很多额外信息,主要是为了减少序列化的大小

- 序列化特点

（1）紧凑简洁：高双使用存储空间
（2）快速：读写数据的额外开销小。
（3）可扩展：随看通信协议的升级而可升级
（4）互操作：支持多语言的交互



## 2. 使用步骤

- Writable接口：
  - write（ Dataoutput out）：对象的序列化
  - readFields（ DataInput in）：对象的反序列化
- 序列化步骤：
  1. 实现Writable接口
  2. 在类中提供无参构造器,反序列化时会反射调用
  3. write方法中写出field顺序,与readFields读取的fields顺序要保持一致
  4. 一般也会重写tostring方法,属性用制表符分割,在reduce输出的KV中如果包含当前类的对象,会用tostring进行打印

![image-20200415094717451](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415094717451.png)



## 3. 案例实操

原数据:

![image-20200415102130077](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415102130077.png)

需求与分析

![image-20200415102307901](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415102307901.png)

使用手机号作为key,能保证相同手机号数据最终能进入同一reduce方法,并实现汇总

使用一个bean对象作为value,bean对象是对某个手机号的上行，下行，总流量的封装

如果在mr过程中，使用了一个bean对象作为k或者v，要保证bean对象是支持序列化和反序列化的

### 3.1 编写流量统计的Bean对象

对每个手机号的上行,下行,总流量的封装

以后定义属性时,尽量定义为包装类型

toString方法直接打印需要数据,用\t分割开来

```java
package com.atguigu.mapreduce.flowsum;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import org.apache.hadoop.io.Writable;

// 1 实现writable接口
public class FlowBean implements Writable{

	private long upFlow;//上行流量
	private long downFlow;//下行流量
	private long sumFlow;//总流量
	
	//2  反序列化时，需要反射调用空参构造函数，所以必须有
	public FlowBean() {
		super();
	}

	public FlowBean(long upFlow, long downFlow) {
		super();
		this.upFlow = upFlow;
		this.downFlow = downFlow;
		this.sumFlow = upFlow + downFlow;
	}
	
	//3  写序列化方法
	@Override
	public void write(DataOutput out) throws IOException {
		out.writeLong(upFlow);
		out.writeLong(downFlow);
		out.writeLong(sumFlow);
	}
	
	//4 反序列化方法
	//5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致
	@Override
	public void readFields(DataInput in) throws IOException {
		this.upFlow  = in.readLong();
		this.downFlow = in.readLong();
		this.sumFlow = in.readLong();
	}

	// 6 编写toString方法，方便后续打印到文本
	@Override
	public String toString() {
		return upFlow + "\t" + downFlow + "\t" + sumFlow;
	}

	public long getUpFlow() {
		return upFlow;
	}

	public void setUpFlow(long upFlow) {
		this.upFlow = upFlow;
	}

	public long getDownFlow() {
		return downFlow;
	}

	public void setDownFlow(long downFlow) {
		this.downFlow = downFlow;
	}

	public long getSumFlow() {
		return sumFlow;
	}

	public void setSumFlow(long sumFlow) {
		this.sumFlow = sumFlow;
	}
}
```



### 3.2 编写Mapper类

![image-20200415104114627](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415104114627.png)

因为数据不一致,从后往前数

```java
package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class FlowCountMapper extends Mapper<LongWritable, Text, Text, FlowBean>{
	
	FlowBean v = new FlowBean();
	Text k = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {
		
		// 1 获取一行
		String line = value.toString();
		
		// 2 切割字段
		String[] fields = line.split("\t");
		
		// 3 封装对象
		// 取出手机号码
		String phoneNum = fields[1];

		// 取出上行流量和下行流量
		long upFlow = Long.parseLong(fields[fields.length - 3]);
		long downFlow = Long.parseLong(fields[fields.length - 2]);

		k.set(phoneNum);
		v.set(downFlow, upFlow);
		
		// 4 写出
		context.write(k, v);
	}
}
```

也可以这么封装key和value

![image-20200415104437119](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415104437119.png)

![image-20200415104422423](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415104422423.png)



### 3.3 编写Reducer类

```java
package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class FlowCountReducer extends Reducer<Text, FlowBean, Text, FlowBean> {

	@Override
	protected void reduce(Text key, Iterable<FlowBean> values, Context context)throws IOException, InterruptedException {

		long sum_upFlow = 0;
		long sum_downFlow = 0;

		// 1 遍历所用bean，将其中的上行流量，下行流量分别累加
		for (FlowBean flowBean : values) {
			sum_upFlow += flowBean.getUpFlow();
			sum_downFlow += flowBean.getDownFlow();
		}

		// 2 封装对象
		FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow);
		
		// 3 写出
		context.write(key, resultBean);
	}
}
```

封装对象也可这么写:

![image-20200415105148358](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105148358.png)

![image-20200415105253046](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105253046.png)



### 3.4 编写Driver驱动类

```java
package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class FlowsumDriver {

	public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException {
		
// 输入输出路径需要根据自己电脑上实际的输入输出路径设置
args = new String[] { "e:/input/inputflow", "e:/output1" };

		// 1 获取配置信息，或者job对象实例
		Configuration configuration = new Configuration();
		Job job = Job.getInstance(configuration);

		// 6 指定本程序的jar包所在的本地路径
		job.setJarByClass(FlowsumDriver.class);

		// 2 指定本业务job要使用的mapper/Reducer业务类
		job.setMapperClass(FlowCountMapper.class);
		job.setReducerClass(FlowCountReducer.class);

		// 3 指定mapper输出数据的kv类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(FlowBean.class);

		// 4 指定最终输出的数据的kv类型
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(FlowBean.class);
		
		// 5 指定job的输入原始文件所在目录
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```

指定路径另外的写法

![image-20200415105722111](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105722111.png)

### 3.5 常见异常与输出结果

注意路径不能重复,不然会报错

![image-20200415105809977](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105809977.png)

输出结果

![image-20200415110115928](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415110115928.png)