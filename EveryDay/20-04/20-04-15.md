# 20-04-15

# 一、WordCount错误原因及解决方案

## 1. 错误总结

![Snipaste_2020-04-15_08-53-40](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/Snipaste_2020-04-15_08-53-40.png)



## 2. 类型匹配错误

1.类型写错了

2.如果保留父类方法,则进入调用,则直接往里面写数据了

在map万法中避留了super.map（ key,value, context）

![image-20200415084924435](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415084924435.png)

父类的map方法直接往里面写数据了

![image-20200415084845566](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415084845566.png)



## 3. 环境问题

安装c+组件， hadoop.dll放到c:/ windows/ system32等



# 二、WordCount集群测试

## 1. 本地改集群(Linux)

集群运行,更改为HDFS的路径

![image-20200415091711707](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415091711707.png)

打包

![image-20200415091834203](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415091834203.png)

启动集群

![image-20200415092030801](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092030801.png)

![image-20200415092103473](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092103473.png)

上传Maven生成的jar包wc.jar

上传源文件

![image-20200415092432815](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092432815.png)

查看结果

![image-20200415092454140](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092454140.png)



## 2. 本地改集群(Win)

### 2.1 添加conf

在获取conf之前添加具体的集群设置\

![image-20200415092904122](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415092904122.png)

### 2.2 添加jar包

可同上,上传到Linux去运行

也使用maven-package打成一个jar包,win的jar可以直接提交到Yarn上去执行

![image-20200415093016770](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093016770.png)

![image-20200415093032784](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093032784.png)

### 2.3 修改配置

(run/debug configuration)

![image-20200415093309248](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093309248.png)

![image-20200415093123065](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093123065.png)

vm options:谁来操作这个集群,后面传入参数为

### 2.4 输出结果

![image-20200415093343889](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415093343889.png)

## ***多敲几遍*** ***多敲几遍*** ***多敲几遍***!!!



# 二、序列化

## 1. 基础概念

- 序列化是什么

序列化:对象->字节,把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 

反序列化:字节->对象,将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。

- 为什么要序列化

 一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。

- 为什么不用Java的序列化

减少了很多额外信息,主要是为了减少序列化的大小

- 序列化特点

（1）紧凑简洁：高双使用存储空间
（2）快速：读写数据的额外开销小。
（3）可扩展：随看通信协议的升级而可升级
（4）互操作：支持多语言的交互



## 2. 使用步骤

- Writable接口：
  - write（ Dataoutput out）：对象的序列化
  - readFields（ DataInput in）：对象的反序列化
- 序列化步骤：
  1. 实现Writable接口
  2. 在类中提供无参构造器,反序列化时会反射调用
  3. write方法中写出field顺序,与readFields读取的fields顺序要保持一致
  4. 一般也会重写tostring方法,属性用制表符分割,在reduce输出的KV中如果包含当前类的对象,会用tostring进行打印

![image-20200415094717451](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415094717451.png)



## 3. 案例实操

原数据:

![image-20200415102130077](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415102130077.png)

需求与分析

![image-20200415102307901](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415102307901.png)

使用手机号作为key,能保证相同手机号数据最终能进入同一reduce方法,并实现汇总

使用一个bean对象作为value,bean对象是对某个手机号的上行，下行，总流量的封装

如果在mr过程中，使用了一个bean对象作为k或者v，要保证bean对象是支持序列化和反序列化的

### 3.1 编写流量统计的Bean对象

对每个手机号的上行,下行,总流量的封装

以后定义属性时,尽量定义为包装类型

toString方法直接打印需要数据,用\t分割开来

```java
package com.atguigu.mapreduce.flowsum;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import org.apache.hadoop.io.Writable;

// 1 实现writable接口
public class FlowBean implements Writable{

	private long upFlow;//上行流量
	private long downFlow;//下行流量
	private long sumFlow;//总流量
	
	//2  反序列化时，需要反射调用空参构造函数，所以必须有
	public FlowBean() {
		super();
	}

	public FlowBean(long upFlow, long downFlow) {
		super();
		this.upFlow = upFlow;
		this.downFlow = downFlow;
		this.sumFlow = upFlow + downFlow;
	}
	
	//3  写序列化方法
	@Override
	public void write(DataOutput out) throws IOException {
		out.writeLong(upFlow);
		out.writeLong(downFlow);
		out.writeLong(sumFlow);
	}
	
	//4 反序列化方法
	//5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致
	@Override
	public void readFields(DataInput in) throws IOException {
		this.upFlow  = in.readLong();
		this.downFlow = in.readLong();
		this.sumFlow = in.readLong();
	}

	// 6 编写toString方法，方便后续打印到文本
	@Override
	public String toString() {
		return upFlow + "\t" + downFlow + "\t" + sumFlow;
	}

	public long getUpFlow() {
		return upFlow;
	}

	public void setUpFlow(long upFlow) {
		this.upFlow = upFlow;
	}

	public long getDownFlow() {
		return downFlow;
	}

	public void setDownFlow(long downFlow) {
		this.downFlow = downFlow;
	}

	public long getSumFlow() {
		return sumFlow;
	}

	public void setSumFlow(long sumFlow) {
		this.sumFlow = sumFlow;
	}
}
```



### 3.2 编写Mapper类

![image-20200415104114627](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415104114627.png)

因为数据不一致,从后往前数

```java
package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class FlowCountMapper extends Mapper<LongWritable, Text, Text, FlowBean>{
	
	FlowBean v = new FlowBean();
	Text k = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {
		
		// 1 获取一行
		String line = value.toString();
		
		// 2 切割字段
		String[] fields = line.split("\t");
		
		// 3 封装对象
		// 取出手机号码
		String phoneNum = fields[1];

		// 取出上行流量和下行流量
		long upFlow = Long.parseLong(fields[fields.length - 3]);
		long downFlow = Long.parseLong(fields[fields.length - 2]);

		k.set(phoneNum);
		v.set(downFlow, upFlow);
		
		// 4 写出
		context.write(k, v);
	}
}
```

也可以这么封装key和value

![image-20200415104437119](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415104437119.png)

![image-20200415104422423](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415104422423.png)



### 3.3 编写Reducer类

```java
package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class FlowCountReducer extends Reducer<Text, FlowBean, Text, FlowBean> {

	@Override
	protected void reduce(Text key, Iterable<FlowBean> values, Context context)throws IOException, InterruptedException {

		long sum_upFlow = 0;
		long sum_downFlow = 0;

		// 1 遍历所用bean，将其中的上行流量，下行流量分别累加
		for (FlowBean flowBean : values) {
			sum_upFlow += flowBean.getUpFlow();
			sum_downFlow += flowBean.getDownFlow();
		}

		// 2 封装对象
		FlowBean resultBean = new FlowBean(sum_upFlow, sum_downFlow);
		
		// 3 写出
		context.write(key, resultBean);
	}
}
```

封装对象也可这么写:

![image-20200415105148358](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105148358.png)

![image-20200415105253046](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105253046.png)



### 3.4 编写Driver驱动类

```java
package com.atguigu.mapreduce.flowsum;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class FlowsumDriver {

	public static void main(String[] args) throws IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException {
		
// 输入输出路径需要根据自己电脑上实际的输入输出路径设置
args = new String[] { "e:/input/inputflow", "e:/output1" };

		// 1 获取配置信息，或者job对象实例
		Configuration configuration = new Configuration();
		Job job = Job.getInstance(configuration);

		// 6 指定本程序的jar包所在的本地路径
		job.setJarByClass(FlowsumDriver.class);

		// 2 指定本业务job要使用的mapper/Reducer业务类
		job.setMapperClass(FlowCountMapper.class);
		job.setReducerClass(FlowCountReducer.class);

		// 3 指定mapper输出数据的kv类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(FlowBean.class);

		// 4 指定最终输出的数据的kv类型
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(FlowBean.class);
		
		// 5 指定job的输入原始文件所在目录
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));

		// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```

指定路径另外的写法

![image-20200415105722111](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105722111.png)

### 3.5 常见异常与输出结果

注意路径不能重复,不然会报错

![image-20200415105809977](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415105809977.png)

输出结果

![image-20200415110115928](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415110115928.png)

# 14001415睡过头了$$$

# 三、Inputformat的结构

## 1. 源码

![image-20200415141351981](20-04-15.assets/image-20200415141351981.png)

实现类:

![image-20200415141625052](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415141625052.png)

获取切片的方法在fileinputformat里做了具体的实现

![Snipaste_2020-04-15_14-17-15](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/Snipaste_2020-04-15_14-17-15.png)

![image-20200415142336334](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415142336334.png)

## 2. 切片与MapTask并行度决定机制

## $$$1430-1443笔记丢失略微走神

### 2.1 分析

1. 切片个数与MapTask个数的关系:有几个切片就有几个maptask

2. MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。maptask开辟需要资源,所以尽量要让每个maptask处理的数据的量级比较合适。
   - 一个task伴随一次JVM的启动和销毁，当一个job中包含成百上千个task任务时，JVM重用可以使得JVM实例在同一个job中重复使用n次，n的值在mapred-site.xml文件中配置。
   - JVM重用的缺陷是：遇到数据倾斜可能会出问题
3. 到底启用多少个MapTask,需要通过切片去量化
4. 切片的个数跟实际的数据量和块的大小设置有关系
5. ![image-20200415145208130](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415145208130.png)

如果说切片过小的话可能会造成频繁的跨机器读取.起一大堆task花的时间比task运行的时间还长就不太合适了

**![image-20200415145123136](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415145123136.png)**

### 2.2 结论

![image-20200415145418429](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415145418429.png)

切片本质:（offset，length，location）

### 2.3 源码

![image-20200415151612396](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415151612396.png)

![image-20200415151731994](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415151731994.png)

获取任务的详情,然后进行迭代,拿到每一个文件,拿到文件的内容

![image-20200415152003334](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415152003334.png)

splitsize,计算的切片的大小:

![image-20200415152147211](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415152147211.png)

如果要改变切片大小则改变maxsize

![image-20200415152420581](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415152420581.png)

split_slop:1.1

![image-20200415152606132](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415152606132.png)

数据倾斜了的话就不切了(剩余大小/块大小):129M，不生成新的切片，那就是一个task可能跑两个两个机器，相比生成两个task来说，效率高一点

ps.这些数据都可以在配置文件里进行配置

一个切片的信息:

![Snipaste_2020-04-15_15-37-42](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/Snipaste_2020-04-15_15-37-42.png)

## 3. CombineTextInputFormat切片机制

小文件存储,不太常用

![image-20200415155034383](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415155034383.png)

使用

![image-20200415155548587](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415155548587.png)

生成三个切片

![Snipaste_2020-04-15_15-56-14](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/Snipaste_2020-04-15_15-56-14.png)

觉得生成切片多了,可以把切片大小设置得大一些些

## 4. NLineInputFormat

用得不多,用处即多少行设置一个切片

![image-20200415163322858](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415163322858.png)

3行设置为一个切片,如此11行就生成4个切片

## 5. KeyValueTextInputFormat

统计输入文件中每一行的第一个单词相同的行数。

![image-20200415163627385](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415163627385.png)

第一个单词为key,右边的为v

//读取到的一行数据会按照预先设置好的分隔符切开，切开以后左边的作为key，右边的作为va1ue进入到 Mapper中//例如：world ni ho。，设置的分隔符为"，切开以后：K:world  V:ni hao

Mapper

```java
package com.atguigu.mapreduce.KeyValueTextInputFormat;
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class KVTextMapper extends Mapper<Text, Text, Text, LongWritable>{
	
// 1 设置value
   LongWritable v = new LongWritable(1);  
    
	@Override
	protected void map(Text key, Text value, Context context)
			throws IOException, InterruptedException {

// banzhang ni hao
        
        // 2 写出
        context.write(key, v);  
	}
}
```

Reducer

```java
package com.atguigu.mapreduce.KeyValueTextInputFormat;
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class KVTextReducer extends Reducer<Text, LongWritable, Text, LongWritable>{
	
    LongWritable v = new LongWritable();  
    
	@Override
	protected void reduce(Text key, Iterable<LongWritable> values,	Context context) throws IOException, InterruptedException {
		
		 long sum = 0L;  

		 // 1 汇总统计
        for (LongWritable value : values) {  
            sum += value.get();  
        }
         
        //封装
        v.set(sum);  
         
        // 2 输出
        context.write(key, v);  
	}
}
```

Driver

```java
package com.atguigu.mapreduce.keyvaleTextInputFormat;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class KVTextDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		
		Configuration conf = new Configuration();
		// 设置切割符***
	conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, " ");
		// 1 获取job对象
		Job job = Job.getInstance(conf);
		
		// 2 设置jar包位置，关联mapper和reducer
		job.setJarByClass(KVTextDriver.class);
		job.setMapperClass(KVTextMapper.class);
job.setReducerClass(KVTextReducer.class);
				
		// 3 设置map输出kv类型
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(LongWritable.class);

		// 4 设置最终输出kv类型
		job.setOutputKeyClass(Text.class);
job.setOutputValueClass(LongWritable.class);
		
		// 5 设置输入输出数据路径
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		
		// 设置输入格式
	job.setInputFormatClass(KeyValueTextInputFormat.class);
		
		// 6 设置输出数据路径
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		// 7 提交job
		job.waitForCompletion(true);
	}
}
```

conf因为原方法过时,因而改成:

![image-20200415164508379](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200415164508379.png)

## 6.自定义InputFormat

一般不会用,多修改读数据的规则

公司里面基本上是基于原有项目代码进行优化，做功能更改，那些代码基本上经过内部自定义封装，学会看源码超超超级重要啊