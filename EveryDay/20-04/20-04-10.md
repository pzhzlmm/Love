# 伪分布式配置$$$

## 配置HDFS

9:00-9:30

所有hadoop的配置文件都在:etc/hadoop目录下(/opt/module/hadoop-3.1.3/etc/hadoop)

### 配置hadoop-env.sh

配置javahome

![image-20200410110449328](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410110449328.png)

### 配置core-site.xml

![image-20200410110418474](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410110418474.png)

### 配置hdfs-site.xml

指定副本的数量(可略)

vim hadf-site.xml

dfs.replication

![image-20200410110611498](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410110611498.png)

伪分布式不需要多个副本

![image-20200410110631904](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410110631904.png)

## 格式化

第一次配置完成后格式化,后续直接启动即可

命令:hdfs namenode -format

完成界面:

![image-20200410093123860](20-04-10.assets/image-20200410093123860.png)

多了data目录(前面配置的)和logs目录(hadoop运行期间的存储目录,出问题时可以进行查看)

![image-20200410110924856](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410110924856.png)

## 启动namenode

hdfs --daemon start namenode

启动jps帮我们查看所有java的进程:

![image-20200410111810080](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410111810080.png)

启动的namenode实际上一个java的进程,实质上是一个java类

![image-20200410111826865](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410111826865.png)

访问namenode

hadoop给了我们个web页面可以访问它

9820是hadoop内部使用的端口号,外部访问是使用端口号9870

![image-20200410111909204](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410111909204.png)

hdfs提供查看文件系统的工具

![image-20200410111921770](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410111921770.png)

![image-20200410102543786](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410102543786.png)

## 启动datanode

hdfs --daemon start datanode

![image-20200410093734646](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410093734646.png)

同样也是Java类

![image-20200410111958874](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410111958874.png)

## 建立目录$$$

hdfs dfs -mkdir /input

查看

![image-20200410094007645](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410094007645.png)

上传文件

![image-20200410094128675](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410094128675.png)

指定输出路径 9:42$$$

![image-20200410112407425](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410112407425.png)

等同于

![image-20200410095443237](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410095443237.png)

查看结果

![image-20200410094246790](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410094246790.png)

![image-20200410094304598](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410094304598.png)

hdfs的文件实质上存储到了data里面

data/temp/dfs:

![image-20200410094822102](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410094822102.png)

文件路径

![image-20200410095031903](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410095031903.png)

![image-20200410094921687](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410094921687.png)

内部维护的一个文件

hadoop自身的文件系统

![image-20200410095001936](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410095001936.png)

hdfs提供了web端,能看到上传的具体文件以及目录

## 配置Yarn$$$

复制![image-20200410104359786](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410104359786.png)

把这部分内容复制到configuration![image-20200410104416177](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410104416177.png)

指定mapper

![image-20200410104632577](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410104632577.png)

启动

![image-20200410120103787](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410120103787.png)

yarn界面

![Snipaste_2020-04-10_10-51-30](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/Snipaste_2020-04-10_10-51-30-1586491224177.png)

测试

![image-20200410115934407](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410115934407.png)

看到正在跑的mn程序

![image-20200410114121039](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410114121039.png)

![image-20200410114229695](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200410114229695.png)

linux调度这个资源,转成yarn去调度这个资源

# 配置文件说明

Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。

（1）默认配置文件：

| 要获取的默认文件     | 文件存放在Hadoop的jar包中的位置                            |
| -------------------- | ---------------------------------------------------------- |
| [core-default.xml]   | hadoop-common-3.1.3.jar/ core-default.xml                  |
| [hdfs-default.xml]   | hadoop-hdfs-3.1.3.jar/ hdfs-default.xml                    |
| [yarn-default.xml]   | hadoop-yarn-common-3.1.3.jar/ yarn-default.xml             |
| [mapred-default.xml] | hadoop-mapreduce-client-core-3.1.3.jar/ mapred-default.xml |

大部分可以直接用,可以直接改,	

（2）自定义配置文件：

​	***\*core-site.xml\*******\*、\*******\*hdfs-site.xml\*******\*、\*******\*yarn-site.xml\*******\*、\*******\*mapred-site.xml\****四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。