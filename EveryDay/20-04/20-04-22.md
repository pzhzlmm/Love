# 20-04-22

HA

# 一、HA概述

所谓HA（High Availablity），即高可用（7*24小时不中断服务）。

保证一直能对外提供服务

zk有内部选举机制,不需要HA

单点故障:一个故障,整个集群无法使用了,如hdfs的NN,Yarn的RN

如何消除单点故障:通过多个NameNode,正在对外Active,等待Standby

文档:

地址:https://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html

![image-20200422101946604](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422101946604.png)

hadoop2之前会有单点故障,只有一个NN,一旦NN故障,整个集群不可用

Prior to Hadoop 2.0.0, the NameNode was a single point of failure (SPOF) in an HDFS cluster. Each cluster had a single NameNode, and if that machine or process became unavailable, the cluster as a whole would be unavailable until the NameNode was either restarted or brought up on a separate machine.

![image-20200422102235617](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422102235617.png)

The HDFS High Availability feature addresses the above problems by providing the option of running two (and as of 3.0.0 more than two) redundant NameNodes in the same cluster in an Active/Passive configuration with a hot standby. This allows a fast failover to a new NameNode in the case that a machine crashes, or a graceful administrator-initiated failover for the purpose of planned maintenance.

一个正常运行,另外一个作为热备,当正常运行的故障的时候,热备迅速替换原有的,因而要保证正常运行的和热备的数据一致性,因而编辑日志要写到共享存储的数据里

所以我们要提供JournalNodes来保证数据保持一致:you should run an odd number of JNs, (i.e. 3, 5, 7, etc.). Note that when running with N JournalNodes, the system can tolerate at most (N - 1) / 2 failures and continue to function normally.也用了pds算法,也是有多个,组成JN的集群,因而哪怕一个故障,也有同ZK的机制进行备份(只要节点数在 (N - 1) / 2 以上即可)

Automatic Failover:自动故障转移,其中一个故障,能把其中的一个替换另外的去工作,全程不需要人类参与

所需条件

```shell
Failure detection - each of the NameNode machines in the cluster maintains a persistent session in ZooKeeper. If the machine crashes, the ZooKeeper session will expire, notifying the other NameNode(s) that a failover should be triggered.

Active NameNode election - ZooKeeper provides a simple mechanism to exclusively elect a node as active. If the current active NameNode crashes, another node may take a special exclusive lock in ZooKeeper indicating that it should become the next active.

The ZKFailoverController (ZKFC) is a new component which is a ZooKeeper client which also monitors and manages the state of the NameNode. Each of the machines which runs a NameNode also runs a ZKFC, and that ZKFC is responsible for:

Health monitoring - the ZKFC pings its local NameNode on a periodic basis with a health-check command. So long as the NameNode responds in a timely fashion with a healthy status, the ZKFC considers the node healthy. If the node has crashed, frozen, or otherwise entered an unhealthy state, the health monitor will mark it as unhealthy.#健康管理

ZooKeeper session management - when the local NameNode is healthy, the ZKFC holds a session open in ZooKeeper. If the local NameNode is active, it also holds a special “lock” znode. This lock uses ZooKeeper’s support for “ephemeral” nodes; if the session expires, the lock node will be automatically deleted.

ZooKeeper-based election - if the local NameNode is healthy, and the ZKFC sees that no other node currently holds the lock znode, it will itself try to acquire the lock. If it succeeds, then it has “won the election”, and is responsible for running a failover to make its local NameNode active. The failover process is similar to the manual failover described above: first, the previous active is fenced if necessary, and then the local NameNode transitions to active state.
```

# 二、HDFS-HA手动故障转移

## 1. 工作要点

1.元数据管理方式需要改变

- 内存中各自保存一份元数据；
- Edits日志只有Active状态的NameNode节点可以做写操作(其他只能读取)
- 两个NameNode都可以读取Edits；
- 共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）

2.必须保证多NameNode之间能够ssh无密码登录

3.隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务

## 2. 搭建

![image-20200422104125751](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422104125751.png)

1.在opt目录下创建一个ha文件夹

[atguigu@hadoop202 opt]$ mkdir ha

2.将/opt/module/下的 hadoop-3.1.3拷贝到/opt/ha目录下

[atguigu@hadoop202 opt] cp -r hadoop-3.1.3 ha

![image-20200422104210440](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422104210440.png)

![image-20200422104223983](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422104223983.png)

3.配置core-site.xml

这些配置都能在官方找到

原有的这些配置先删除

```xml
<configuration>
<!-- 把多个NameNode的地址组装成一个集群mycluster -->
		<property>
			<name>fs.defaultFS</name>
        	<value>hdfs://mycluster</value>
		</property>
	
	<!-- 指定hadoop运行时产生文件的存储目录 -->
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/opt/module/ha/hadoop-3.1.3/data/tmp</value>
		</property>
   <!-- 声明journalnode服务器存储目录-->
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>file://${hadoop.tmp.dir}/jn</value>
	</property>
</configuration>
```

4.配置hdfs-site.xml(原有的都删除)

```xml
<configuration>
	<!-- 完全分布式集群名称 -->
	<property>
		<name>dfs.nameservices</name>
		<value>mycluster</value>
	</property>
  <!-- NameNode数据存储目录 -->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.tmp.dir}/name</value>
  </property>
 <!-- DataNode数据存储目录 -->
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.tmp.dir}/data</value>
  </property>

	<!-- 集群中NameNode节点都有哪些 -->
	<property>
		<name>dfs.ha.namenodes.mycluster</name>
		<value>nn1,nn2,nn3</value>
	</property>

	<!-- nn1的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn1</name>
		<value>hadoop102:9000</value>
	</property>

	<!-- nn2的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn2</name>
		<value>hadoop103:9000</value>
	</property>
	<!-- nn3的RPC通信地址 -->
	<property>
		<name>dfs.namenode.rpc-address.mycluster.nn3</name>
		<value>hadoop104:9000</value>
	</property>


	<!-- nn1的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn1</name>
		<value>hadoop102:9870</value>
	</property>

	<!-- nn2的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn2</name>
		<value>hadoop103:9870</value>
	</property>
	<!-- nn3的http通信地址 -->
	<property>
		<name>dfs.namenode.http-address.mycluster.nn3</name>
		<value>hadoop104:9870</value>
	</property>

	<!-- 指定NameNode元数据在JournalNode上的存放位置 -->
    <!-- 如何去查找到JournalNode通过什么协议,到哪里去找 -->
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
	<value>qjournal://hadoop102:8485;hadoop103:8485;hadoop104:8485/mycluster</value>
	</property>

	<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence</value>
	</property>

	<!-- 使用隔离机制时需要ssh无秘钥登录-->
	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/home/atguigu/.ssh/id_rsa</value>
	</property>

	<!-- 访问代理类：client用于确定哪个NameNode为Active -->
	<property>		<name>dfs.client.failover.proxy.provider.mycluster</name>
	<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>
</configuration>
```

5.拷贝配置好的hadoop环境到其他节点

分发:

删除data和logs

hadoop的环境变量:

![image-20200422112239042](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422112239042.png)

改成

![image-20200422112254017](20-04-22.assets/image-20200422112254017-1587525811224.png)

修改后不要忘记source一下

![image-20200422112312897](https://sumomoriaty.oss-cn-beijing.aliyuncs.com/image-20200422112312897.png)

## 3. 启动

1.	在各个JournalNode节点上，输入以下命令启动journalnode服务

hdfs --daemon start journalnode

2.	在[nn1]上，对其进行格式化，并启动

hdfs namenode -format

hdfs --daemon start namenode

3.	在[nn2]和[nn3]上，同步nn1的元数据信息

hdfs namenode -bootstrapStandby

4.	启动[nn2] 和 [nn3]

hdfs --daemon start namenode

5.	查看web页面显示，102 103 104 都为standby状态

6.	启动所有datanode

hdfs --daemon start datanode

7.	将[nn1]切换为Active

hdfs haadmin -transitionToActive nn1

\8. 是否Active

hdfs haadmin -getServiceState nn1

\9. kill掉Active的NameNode，进行手动故障转移